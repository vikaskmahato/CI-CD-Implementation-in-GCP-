steps:
  # - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
  #   entrypoint: 'bash'
  #   args:
  #     - '-c'
  #     - |
  #       # Go to the folder containing the SQL file
  #       cd sql
  #       # Run BigQuery query using 'bq'
  #       bq query --use_legacy_sql=false --format=none \
  #         "$(cat view_script.sql)"
    # - '--file=sql/view_script.sql'
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        cd /workspace
        echo "Fetching changed files..."
        git fetch --depth=2 origin main
        # Get the list of changed files between the latest commit and the previous commit
        CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD)
        echo "Changed files: $CHANGED_FILES"

        # Save the list of changed files to a workspace file
        # echo "$CHANGED_FILES" > /workspace/changed_files.txt
        
        # CHANGED_FILES=$(cat /workspace/changed_files.txt)
        # Initialize an empty list of files to process
        SQL_FILES_TO_PROCESS=""

        # Identify which files are SQL files in specific folders
        for file in $CHANGED_FILES; do
          if [[ "$file" =~ .*\.sql$ ]]; then
          # if [[ "$file" =~ ^source/.*\.sql$ || "$file" =~ ^historical/.*\.sql$ || "$file" =~ ^test/.*\.sql$ ]]; then
            SQL_FILES_TO_PROCESS="$SQL_FILES_TO_PROCESS $file"
          fi
        done

        # Check if there are any SQL files to process
        if [ -z "$SQL_FILES_TO_PROCESS" ]; then
          echo "No SQL files changed. Exiting."
          exit 0
        else
          echo "Changed SQL files: $SQL_FILES_TO_PROCESS"
        fi

  # Run each SQL file in BigQuery
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    entrypoint: 'bash'
    args:
      - '-c'
      - |
        # For each SQL file, run the corresponding query in BigQuery
        for file in $SQL_FILES_TO_PROCESS; do
          echo "Running SQL query from file: $file"
          bq query --use_legacy_sql=false --format=none \
            "$(cat $file)"
        done


timeout: '1600s'
options:
  logging: CLOUD_LOGGING_ONLY
